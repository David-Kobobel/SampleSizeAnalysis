{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "439ff65e",
   "metadata": {},
   "source": [
    "============================================================\n",
    "Host Annotation Pipeline for PubMed DOIs\n",
    "============================================================\n",
    "Description:\n",
    "   Annotates PubMed articles with MeSH host species terms based on DOIs.\n",
    "   Uses NCBI Entrez utilities to retrieve PMIDs and MeSH terms, \n",
    "   applies robust filters to exclude non-host or irrelevant taxa, \n",
    "   and writes results incrementally to a CSV file.\n",
    "\n",
    "Environment:\n",
    "   Python 3.10+ \n",
    "   Dependencies: numpy, requests, urllib, biopython, pandas, re, json, time, xml.etree.ElementTree\n",
    "\n",
    "Usage:\n",
    "   1. Set Entrez.email (required by NCBI).\n",
    "   2. Provide a DataFrame (df) containing a \"DOI\" column.\n",
    "   3. Run analyze_dois_from_df(df, output_path=\"doi_host_species_output.csv\").\n",
    "============================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2f3c91",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c005dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd              # For data loading and manipulation\n",
    "import numpy as np               # For numerical operations\n",
    "import re                        # For regex-based string validation\n",
    "import requests                  # For validating DOI URLs or checking links\n",
    "import urllib                    # For URL parsing/validation\n",
    "\n",
    "# validation and web checks\n",
    "from urllib.parse import urlparse   # For checking valid URL structures\n",
    "from urllib.parse import quote      # For URL encoding\n",
    "from urllib.parse import unquote    # For URL encoding\n",
    "from Bio import Entrez              # For validating NCBI accession codes\n",
    "\n",
    "# for reading MeSH Data\n",
    "import xml.etree.ElementTree as ET   # Parse and create XML\n",
    "import json                          # Serialize and parse JSON\n",
    "import os                            # Interact with the operating system\n",
    "import time                          # Functions for working with time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc5f6ea",
   "metadata": {},
   "source": [
    "# Import csv:\n",
    "\n",
    "https://docs.google.com/spreadsheets/d/1tBjpV_GoXIjx4_3o73Qml0h-BzFBZTD5bc3QHx1l1_4/edit?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550f4784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load Google Sheet ---\n",
    "sheet_id = \"1tBjpV_GoXIjx4_3o73Qml0h-BzFBZTD5bc3QHx1l1_4\"\n",
    "sheet_name = \"Draft of Final Data Sheet\"\n",
    "encoded_sheet = quote(sheet_name)\n",
    "\n",
    "csv_url = f\"https://docs.google.com/spreadsheets/d/{sheet_id}/gviz/tq?tqx=out:csv&sheet={encoded_sheet}\"\n",
    "\n",
    "df = pd.read_csv(csv_url, header=0)\n",
    "\n",
    "df.columns = (\n",
    "    df.columns.astype(str)\n",
    "      .str.strip()\n",
    "      .str.replace(\"\\ufeff\", \"\", regex=False)\n",
    ")\n",
    "\n",
    "# --- Basic cleaning ---\n",
    "if \"StudyLink\" in df.columns:\n",
    "    df = df[~df[\"StudyLink\"].astype(str).str.contains(\"END\", na=False)]\n",
    "if \"Author1Author2name (LastName1LastName2)\" in df.columns:\n",
    "    df = df.dropna(subset=[\"StudyLink\", \"Author1Author2name (LastName1LastName2)\"], how=\"all\")\n",
    "\n",
    "# Inspect header\n",
    "print(f\"Total rows: {df.shape[0]}, Total columns: {df.shape[1]}\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b11c4c",
   "metadata": {},
   "source": [
    "# MeSH Taxonmic Mappings\n",
    "\n",
    "Download MeSH Data (https://www.nlm.nih.gov/databases/download/mesh.html).\n",
    "Medical Subject Headings (MeSH) is a hierarchically-organized terminology for indexing and cataloging of biomedical information. It is used for the indexing of PubMed and other NLM databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9b782a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your MeSH XML path\n",
    "desc_path = r\"C:\\Users\\emily\\MMC-visualizations\\users\\emily-chen-song\\desc2025.xml\"\n",
    "\n",
    "# Parse XML\n",
    "tree = ET.parse(desc_path)\n",
    "root = tree.getroot()\n",
    "\n",
    "# Extract descriptors with TreeNumbers starting with B01 (organism taxonomy)\n",
    "records = []\n",
    "for descriptor in root.findall(\".//DescriptorRecord\"):\n",
    "    name_elem = descriptor.find(\".//DescriptorName/String\")\n",
    "    tree_numbers = descriptor.findall(\".//TreeNumberList/TreeNumber\")\n",
    "\n",
    "    if name_elem is not None:\n",
    "        name = name_elem.text\n",
    "        for tn in tree_numbers:\n",
    "            if tn.text and tn.text.startswith(\"B01\"):\n",
    "                records.append({\"Name\": name, \"TreeNumber\": tn.text})\n",
    "\n",
    "# Create DataFrame\n",
    "df_species = pd.DataFrame(records)\n",
    "\n",
    "# Save to CSV\n",
    "df_species.to_csv(\"meSH_taxonomic_species_from_xml.csv\", index=False)\n",
    "\n",
    "# Create lowercase mapping dictionary for host detection script\n",
    "species_dict = {name.lower(): name for name in df_species[\"Name\"]}\n",
    "with open(\"meSH_taxonomic_species_from_xml.json\", \"w\") as f:\n",
    "    json.dump(species_dict, f)\n",
    "\n",
    "print(f\"Extracted {len(species_dict)} taxonomic MeSH terms from XML.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c899b0",
   "metadata": {},
   "source": [
    "# Extract Relevant Host MeSH Terms from DOIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14bdbd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Settings ---\n",
    "# Replace with your email\n",
    "Entrez.email = \"your_email@example.com\"\n",
    "\n",
    "# Set TRUE if only want human hosts\n",
    "ALLOW_HUMANS_ONLY = False  \n",
    "\n",
    "# --- Load MeSH species dictionary ---\n",
    "with open(\"meSH_taxonomic_species_from_xml.json\", \"r\") as f:\n",
    "    MESH_SPECIES_DICT = json.load(f)\n",
    "MESH_SPECIES_DICT = {k.strip().casefold(): v for k, v in MESH_SPECIES_DICT.items()}\n",
    "\n",
    "# --- DOI cleaner ---\n",
    "DOI_PAT = re.compile(r'10\\.\\d{4,9}/[^\\s\"<>]+', re.IGNORECASE)\n",
    "def clean_doi(x):\n",
    "    if x is None:\n",
    "        return None\n",
    "    s = str(x).strip()\n",
    "    if not s or s.lower() in {\"nan\", \"none\"}:\n",
    "        return None\n",
    "    s = unquote(s)\n",
    "    m = DOI_PAT.search(s)\n",
    "    if not m:\n",
    "        return None\n",
    "    return m.group(0).rstrip(').,;]')\n",
    "\n",
    "# --- Filtering helpers ---\n",
    "def norm(s: str) -> str:\n",
    "    return re.sub(r'\\s+', ' ', str(s).strip()).casefold()\n",
    "\n",
    "EXCLUDED_EXACT = {\n",
    "    # generic categories\n",
    "    \"animals\", \"organisms\", \"male\", \"female\", \"child\", \"children\", \"adult\",\n",
    "    \"infant\", \"pregnancy\", \"pregnant\", \"vertebrates\", \"mammals\", \"eukaryota\",\n",
    "    \"livestock\", \"newborn\", \"wild\", \"domestic\",\n",
    "    # lab or farm animals\n",
    "    \"rats\", \"rats, sprague-dawley\", \"mice\", \"mice, inbred balb c\",\n",
    "    \"swine\", \"dogs\", \"cats\", \"pets\", \"cattle\", \"sheep\", \"goats\", \"pigs\",\n",
    "}\n",
    "\n",
    "EXCLUDED_PATTERNS = [\n",
    "    # existing generic ones\n",
    "    r\"\\binbred\\b\", r\"\\bknockout\\b\", r\"\\bnude\\b\",\n",
    "    r\"\\bnew world\\b\", r\"\\btransgenic\\b\", r\"\\bwild[- ]?type\\b\",\n",
    "    # plants / fungi / protozoa / animals often irrelevant\n",
    "    r\"blastocystis\", r\"prunus\", r\"pyroglyphid\",\n",
    "    # lab & domestic animals\n",
    "    r\"\\brat[s]?\\b\", r\"\\bmouse\\b\", r\"\\bmice\\b\",\n",
    "    r\"\\bswine\\b\", r\"\\bpig[s]?\\b\", r\"\\bpork\\b\",\n",
    "    r\"\\bcow[s]?\\b\", r\"\\bcattle\\b\", r\"\\bovine\\b\", r\"\\bbovine\\b\",\n",
    "    r\"\\bsheep\\b\", r\"\\bgoat[s]?\\b\", r\"\\bcanine\\b\", r\"\\bdog[s]?\\b\",\n",
    "    r\"\\bcat[s]?\\b\", r\"\\bfeline\\b\", r\"\\bpoultry\\b\", r\"\\bchicken[s]?\\b\",\n",
    "    r\"\\bhamster[s]?\\b\", r\"\\brabbit[s]?\\b\", r\"\\bmonkey[s]?\\b\",\n",
    "    r\"\\bmacaca\\b\", r\"\\bvervet\\b\", r\"\\bmarmoset\\b\",\n",
    "    # umbrella terms\n",
    "    r\"\\banimal[s]?\\b\", r\"\\blivestock\\b\", r\"\\bnewborn\\b\",\n",
    "    # pets / companion animals\n",
    "    r\"\\bpet[s]?\\b\"\n",
    "]\n",
    "\n",
    "EX_PAT = re.compile(\"|\".join(EXCLUDED_PATTERNS), flags=re.IGNORECASE)\n",
    "ALLOW_EXACT = {\"homo sapiens\", \"human\", \"humans\"}\n",
    "\n",
    "def host_term_is_allowed(descriptor_text: str, qualifiers: list[str]) -> bool:\n",
    "    dnorm = norm(descriptor_text)\n",
    "    if not dnorm:\n",
    "        return False\n",
    "    if dnorm in EXCLUDED_EXACT:\n",
    "        return False\n",
    "    if EX_PAT.search(dnorm):\n",
    "        return False\n",
    "    for q in qualifiers:\n",
    "        if EX_PAT.search(q):\n",
    "            return False\n",
    "    if ALLOW_HUMANS_ONLY:\n",
    "        return dnorm in ALLOW_EXACT\n",
    "    return True\n",
    "\n",
    "# --- PubMed helpers ---\n",
    "def get_pmid_from_doi(doi):\n",
    "    try:\n",
    "        h = Entrez.esearch(db=\"pubmed\", term=doi, retmode=\"xml\")\n",
    "        rec = Entrez.read(h)\n",
    "        h.close()\n",
    "        return rec[\"IdList\"][0] if rec[\"IdList\"] else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def get_species_from_pmid(pmid):\n",
    "    try:\n",
    "        h = Entrez.efetch(db=\"pubmed\", id=pmid, retmode=\"xml\")\n",
    "        xml_data = h.read()\n",
    "        h.close()\n",
    "        root = ET.fromstring(xml_data)\n",
    "        species_out = []\n",
    "\n",
    "        for mh in root.findall(\".//MeshHeading\"):\n",
    "            desc_el = mh.find(\"DescriptorName\")\n",
    "            if desc_el is None or not desc_el.text:\n",
    "                continue\n",
    "            descriptor_text = desc_el.text.strip()\n",
    "            qualifiers = [q.text.strip() for q in mh.findall(\"QualifierName\") if q.text]\n",
    "            if not host_term_is_allowed(descriptor_text, qualifiers):\n",
    "                continue\n",
    "            key = norm(descriptor_text)\n",
    "            if key in MESH_SPECIES_DICT:\n",
    "                species_out.append(MESH_SPECIES_DICT[key])\n",
    "        # unique preserve order\n",
    "        seen, uniq = set(), []\n",
    "        for s in species_out:\n",
    "            if s not in seen:\n",
    "                seen.add(s)\n",
    "                uniq.append(s)\n",
    "        return uniq if uniq else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# --- Main analysis (LIVE CSV writing) ---\n",
    "def analyze_dois_from_df(df, output_path=\"doi_host_species_output.csv\"):\n",
    "    # detect DOI column\n",
    "    doi_col = next((c for c in df.columns if c.strip().lower() == \"doi\"), None)\n",
    "    if doi_col is None:\n",
    "        raise ValueError(f\"No 'DOI' column found. Columns: {list(df.columns)}\")\n",
    "\n",
    "    abs_path = os.path.abspath(output_path)\n",
    "    header_needed = not os.path.exists(abs_path)\n",
    "\n",
    "    with open(abs_path, \"a\", encoding=\"utf-8\") as f:\n",
    "        if header_needed:\n",
    "            f.write(\"DOI,PMID,Host Species\\n\")\n",
    "\n",
    "        for idx, row in df.iterrows():\n",
    "            raw_doi = row.get(doi_col)\n",
    "            doi = clean_doi(raw_doi)\n",
    "            if not doi:\n",
    "                print(f\"[skip] Row {idx}: invalid DOI ({raw_doi})\")\n",
    "                continue\n",
    "\n",
    "            print(f\"Processing DOI {idx+1}/{len(df)}: {doi}\")\n",
    "            pmid = get_pmid_from_doi(doi)\n",
    "            species = get_species_from_pmid(pmid) if pmid else None\n",
    "            species_str = \"; \".join(species) if species else \"\"\n",
    "\n",
    "            # write immediately\n",
    "            f.write(f\"{doi},{pmid if pmid else ''},{species_str}\\n\")\n",
    "            f.flush()  # ensure data hits disk immediately\n",
    "           \n",
    "            time.sleep(0.34)\n",
    "\n",
    "    print(f\"Live-updated results saved to: {abs_path}\")\n",
    "\n",
    "# --- Run ---\n",
    "# Output hosts to csv file named \"doi_host_species_output\"\n",
    "analyze_dois_from_df(df, output_path=\"doi_host_species_output.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
